import string
from collections import Counter
import re

def text_analyzer(text):

    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not text or not text.strip():
        print("–û—à–∏–±–∫–∞: —Ç–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
        return
    
    # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—É—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)
    text_clean = text.translate(str.maketrans('', '', string.punctuation))

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞
    words = re.findall(r'\b\w+\b', text_clean.lower())
    
    #–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –≤ —Å–ª—É—á–∞–µ –µ—Å–ª–∏ –Ω–µ—Ç —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ 
    if not words:
        print("–í —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        return
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_words = len(words) #–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤
    total_chars = len(text.replace(' ', '')) #–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤, –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤
    unique_words = len(set(words)) #–ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    
    # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    word_freq = Counter(words)
    common_words = word_freq.most_common(5)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–ª–∏–Ω–µ —Å–ª–æ–≤
    word_lengths = [len(word) for word in words]
    avg_length = sum(word_lengths) / len(word_lengths) #–í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    lexical_diversity = unique_words / total_words * 100  # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print()
    print("*" * 45)
    print()
    print("–ü–û–î–†–û–ë–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê")
    print()
    print("*" * 45)
    print()
    print(f"üìä –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print()
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {total_words:,}")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤): {total_chars:,}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words:,}")
    print(f"   –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {lexical_diversity:.1f}%")
    print()
    print("_" * 45)
    print()
    print(f"\nüèÜ –°–ê–ú–´–ï –ß–ê–°–¢–´–ï –°–õ–û–í–ê:")
    print()
    print("*" * 45)
    print()

    for i, (word, count) in enumerate(common_words, 1):
        percentage = (count / total_words) * 100
        print(f"   {i}. '{word}': {count} —Ä–∞–∑ ({percentage:.1f}%)")
    
    print()
    print("_" * 45)
    print()
    print(f"\nüìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù–´ –°–õ–û–í:")
    print()
    print("*" * 45)
    print()
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {avg_length:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: '{max(words, key=len)}' ({len(max(words, key=len))} —Å–∏–º–≤.)")
    print(f"   –°–∞–º–æ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ: '{min(words, key=len)}' ({len(min(words, key=len))} —Å–∏–º–≤.)")
    print()

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º —Å–ª–æ–≤
    length_distribution = Counter(word_lengths)
    print()
    print("_" * 45)
    print()
    print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–õ–û–í –ü–û –î–õ–ò–ù–ï:")
    print()
    print("*" * 45)
    print()
    for length in sorted(length_distribution.keys()):
        count = length_distribution[length]
        percentage = (count / total_words) * 100
        bar = "‚ñà" * int(percentage / 2)  # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        print(f"   {length:2} —Å–∏–º–≤.: {count:3} —Å–ª–æ–≤ {bar} ({percentage:.1f}%)")
    print()
    print("_" * 45)
    print()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    sample_text = """
    –ë—ã—Å—Ç—Ä–∞—è –∫–æ—Ä–∏—á–Ω–µ–≤–∞—è –ª–∏—Å–∞ –ø—Ä—ã–≥–∞–µ—Ç —á–µ—Ä–µ–∑ –ª–µ–Ω–∏–≤—É—é —Å–æ–±–∞–∫—É. 
    –≠—Ç–∞ –ª–∏—Å–∞ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è –∏ —É–º–Ω–∞—è. –°–æ–±–∞–∫–∞ –∂–µ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –æ—Ç–¥—ã—Ö–∞—Ç—å.
    """
    
    text_analyzer(sample_text)